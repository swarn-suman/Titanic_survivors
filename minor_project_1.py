# -*- coding: utf-8 -*-
"""Minor_Project_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NnZirJp0oMQHdsoJ8yHiq3zkk2fcWrmX
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df=pd.read_csv("/content/drive/MyDrive/Prashant_Singh_Rinex_2023/gender_submission.csv")
df

train=pd.read_csv("/content/drive/MyDrive/Prashant_Singh_Rinex_2023/train.csv")
train

test=pd.read_csv("/content/drive/MyDrive/Prashant_Singh_Rinex_2023/test.csv")
test

train.duplicated().sum()

train.isnull().sum()

train['Pclass'].unique()

sns.barplot(x=train['Survived'],y=train['Age'],errorbar=None)
plt.xticks(rotation=90)
plt.show()

"""**In this representation we can see that most people above the age of 30's have died on the other hand those how are younger survived.**"""

sns.displot(x=train['Age'],kde=True)

"""**In this representation it can be noticed that area of the people above the age of 30 is greater this indicates that the most of the passengers on the ship are older than 30 .Thus most of the passenger who boarded the ship did'nt  survive.**"""

sns.barplot(x=train['Survived'],y=train['SibSp'],errorbar=None)
plt.xticks(rotation=90)
plt.show()

"""**This representtion indicates that the passengers that have the siblings did not survive and thos who dont have one survived.**"""

sns.displot(x=train['SibSp'],kde=True)

"""***This indicates that most of the passengers that where on the ship did not had a sibling as the area of the curve near to the 0 is higher than the area near the 1 ***"""

sns.barplot(x=train['Survived'],y=train['Pclass'],errorbar=None)
plt.xticks(rotation=90)
plt.show()

"""**this indicates that most of the survivors belong to the high passenger class like 1,2 and most of the non surviors belong to passenger class 3**"""

#most of the rows in the cabin columb are emplt so we can not conclude any thing from this columb.So its better to drop this columb.

train.drop(columns=['Cabin'],inplace=True)

train.dropna(inplace=True)

test.dropna(inplace=True)

train

sns.barplot(x=train['Survived'],y=train['Sex'],errorbar=None)
plt.xticks(rotation=90)
plt.show()

"""**This represent that most of the passengers that survived are female.**"""

sns.displot(x=train['Sex'],kde=True)

"""**This representation indicates that mmost of the passengers that boarded the ship are male.**"""

sns.displot(x=train['Parch'],kde=True)

"""**This indicates that for most of the passengers Parch=0**"""

sns.barplot(x=train['Survived'],y=train['Parch'],errorbar=None)
plt.xticks(rotation=90)
plt.show()

"""**This indicates that percent of people who had parch=1 have survived is greater that those who parch=0 and survived**"""

sns.displot(x=train['Survived'],kde=True)

"""**This represent that the number people who boarded the ship and did not survive are greated than those who boarded the ship and did survive.**"""

sns.displot(x=train['Embarked'],kde=True)

"""**this indicates that most of the passengers mounted from  Southampton and then from Cherbourg and least number of passengers mounted from Queenstown**."""

sns.barplot(x=train['Survived'],y=train['Fare'],errorbar=None)
plt.xticks(rotation=90)
plt.show()

"""**This indicates that most of the people who paid high fair survived**"""

train.sample(1)

#After analysing the database we can see that we do not need PassengerId ,Name ,Ticket to predict the out come

from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.metrics import r2_score

X_train=train.drop(columns=['PassengerId','Name','Ticket'])

X_train.drop(columns=['Survived'],inplace=True)
Y_train=train['Survived']

X_test=test.drop(columns=['PassengerId','Name','Ticket'])

y_test=df.drop(columns=['PassengerId'])

X_train.isnull().sum()

step1=ColumnTransformer(transformers=[
    ('col_tnf',OneHotEncoder(sparse_output=False,drop='first'),[0,1,2,4,5])
    ],remainder='passthrough')
step2=KNeighborsClassifier(n_neighbors=5,metric='euclidean')
pipe=Pipeline([('step1',step1),('step2',step2)])
pipe.fit(X_train,Y_train)
y_pred=pipe.predict(X_test)

print("R2 score:",r2_score(y_test,y_pred))